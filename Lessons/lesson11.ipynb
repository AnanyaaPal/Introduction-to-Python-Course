{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61af821c-4037-4f67-a11f-95a15c0696a6",
   "metadata": {},
   "source": [
    "#| label: intro\n",
    "\n",
    "# Course material 10\n",
    "## Lesson 11 (21.12.2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f2938-6c8e-4962-9fff-9ad6ebaf7524",
   "metadata": {},
   "source": [
    "> Disclaimer: Material is taken from \n",
    "> \n",
    "> + mCoding with James Murphy, \"Automated Testing in Python with pytest, tox, and GitHub Actions\" retrieved from [https://www.youtube.com/watch?v=DhUpxWjOhME&t=430s](https://www.youtube.com/watch?v=DhUpxWjOhME&t=430s) (20.12.2023)\n",
    "> + freeCodeCamp with @iamrithmic, \"Pytest Tutorial â€“ How to Test Python Code\" retrieved from [https://www.youtube.com/watch?v=cHYq1MRoyI0](https://www.youtube.com/watch?v=cHYq1MRoyI0) (20.12.2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0377a9-5fa7-4478-8588-392e7cd79d71",
   "metadata": {},
   "source": [
    "## Setting up the folder structure\n",
    "\n",
    "+ Before we start writing tests we have to restructure our folder a bit in order to setup a testing environment\n",
    "+ We start by making some changes in the `pyproject.toml` file. Change the current content with the following:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b64a5b0e-9677-43c4-ae59-f5206f427f6d",
   "metadata": {},
   "source": [
    "# pyproject.toml\n",
    "\n",
    "[build-system]\n",
    "requires = [\"setuptools\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "testpaths = [\n",
    "    \"tests\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee103b-8231-4677-9fc3-10b30d898c1a",
   "metadata": {},
   "source": [
    "+ now we will create a new file, called `setup.cfg` \n",
    "+ The `setup.cfg` file is a configuration file commonly used in Python projects to specify metadata, options, and dependencies for a Python package.\n",
    "    + [metadata]\n",
    "        + name: The name of the project/package, which is \"example_project\" in this case.\n",
    "        + description: A brief description of the project, indicating that it is the \"First example project.\"\n",
    "        + url: The URL of the project's GitHub repository.\n",
    "        + author: The name of the author, specified as \"Florence Bockting.\"\n",
    "        + license: The license under which the project is distributed, specified as the MIT License.\n",
    "        + license_files: The file(s) containing the text of the license, specified as \"LICENSE.\"\n",
    "        + classifiers: Metadata classifiers providing information about the project. It indicates compatibility with Python 3, specifically versions 3.11.7 and 3.12.0.\n",
    "    + [options]\n",
    "        + python_requires: Specifies the minimum Python version required for the project, set to be >=3.6.\n",
    "        + packages: Specifies the Python packages to include in the distribution, and in this case, it includes the \"example_project\" package.\n",
    "        + install_requires: Lists the dependencies required by the project, including specific versions for libraries such as numpy, pandas, polars, pyarrow, scipy, seaborn, matplotlib, and Requests.\n",
    "    + [options.extras_require]\n",
    "        + testing: Specifies additional dependencies required for testing. These include tox, pytest, and pytest-cov.\n",
    "        + docs: Specifies additional dependencies required for generating documentation. These include sphinx, sphinx-book-theme, numpydoc, myst_nb, and sphinx_design.\n",
    "\n",
    "+ create a new file called `setup.cfg` in your main example-project directory and copy&paste the following code into it"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f50979a-a398-45e9-a69c-87c7e00cc771",
   "metadata": {},
   "source": [
    "# setup.cfg\n",
    "\n",
    "[metadata]\n",
    "name = example_project\n",
    "description = First example project\n",
    "url = https://github.com/florence-bockting/example-project\n",
    "author = Florence Bockting\n",
    "license = MIT\n",
    "license_files = LICENSE\n",
    "classifiers =\n",
    "    Programming Language :: Python :: 3\n",
    "    Programming Language :: Python :: 3.11.7\n",
    "    Programming Language :: Python :: 3.12.0\n",
    "\n",
    "[options]\n",
    "python_requires = >=3.6\n",
    "packages = example_project\n",
    "install_requires =\n",
    "    numpy >= 1.23\n",
    "    pandas >= 1.4\n",
    "    polars >= 0.19.19\n",
    "    pyarrow >= 11.0.0\n",
    "    scipy >= 1.8\n",
    "    seaborn >= 0.11\n",
    "    matplotlib >= 3.5\n",
    "    Requests >= 2.31.0\n",
    "\n",
    "[options.extras_require]\n",
    "testing =\n",
    "    tox >= 3.24\n",
    "    pytest >= 6.0\n",
    "    pytest-cov >= 2.10.0\n",
    "\n",
    "docs =\n",
    "    sphinx >= 5.1.0\n",
    "    sphinx-book-theme>=0.2.0\n",
    "    numpydoc >= 1.2.1\n",
    "    myst_nb >= 0.13.1\n",
    "    sphinx_design >= 0.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be13bec-f083-437f-ace8-9319cb53aa2a",
   "metadata": {},
   "source": [
    "+ then we create a new file called `tox.ini`\n",
    "+ The `tox.ini` configuration file is used to define and run multiple test environments for your Python project.\n",
    "    + Tox Configuration:\n",
    "        + `minversion = 3.8.0`: Specifies the minimum required version of tox.\n",
    "        + `envlist = py3117, py3120`: Defines the list of environment names to be created and tested.\n",
    "        + `isolated_build = true`: Indicates that tox should create isolated environments.\n",
    "    + GitHub Actions Configuration:\n",
    "        + Provides a mapping between Python versions specified in the GitHub Actions workflow and the environment names defined in tox.\n",
    "        + For example, if the GitHub Actions workflow specifies Python 3.11.7, it will use the py3117 environment.\n",
    "    + Test Environment Configuration:\n",
    "        + `setenv`: Sets environment variables; in this case, it sets PYTHONPATH to the project directory.\n",
    "        + `deps`: Installs dependencies specified in the `requirements_dev.txt` file.\n",
    "        + `commands`: Executes the specified command (`pytest --basetemp={envtmpdir}`) for running tests. The {envtmpdir} is a directory that tox provides for temporary files related to the specific environment.\n",
    "\n",
    "The following tox.ini file sets up two test environments (py3117 and py3120) and uses them to run tests using pytest. The GitHub Actions section helps map Python versions used in GitHub Actions to tox environment names. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cce2948d-ebfb-4f6a-b036-4963fdcc6bfa",
   "metadata": {},
   "source": [
    "# tox.ini\n",
    "\n",
    "[tox]\n",
    "minversion = 3.8.0\n",
    "envlist = py3117, py3120\n",
    "isolated_build = true\n",
    "\n",
    "[gh-actions]\n",
    "python =\n",
    "   3.11.7: py3117\n",
    "   3.12.0: py3120\n",
    "\n",
    "[testenv]\n",
    "setenv =\n",
    "    PYTHONPATH = {toxinidir}\n",
    "deps =\n",
    "    -r{toxinidir}/requirements_dev.txt\n",
    "commands =\n",
    "    pytest --basetemp={envtmpdir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46228e9-4d99-40f4-a30c-1a61efd3f399",
   "metadata": {},
   "source": [
    "+ The requirements_dev.txt file contains development dependencies needed for testing. Let's create this file now.\n",
    "+ create a `requirements_dev.txt` and copy&paste the following content"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee81bac0-13b7-4363-adc9-a428fa782a30",
   "metadata": {},
   "source": [
    "# requirements_dev.txt\n",
    "\n",
    "tox>=4.10.0\n",
    "pytest>=7.2.0\n",
    "pytest-cov>=4.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3521214-afc2-46e7-9850-c9c0280f0c49",
   "metadata": {},
   "source": [
    "+ and we have to create a new workflow such that github is informed about running out tests\n",
    "+ create in `.github/workflows/` a new file called `tests.yml` and copy&paste the following content:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b73ef875-a96e-4f89-9ad9-64ab45e75ec8",
   "metadata": {},
   "source": [
    "# .github/workflows/test.yml\n",
    "\n",
    "name: Tests\n",
    "\n",
    "# The workflow is triggered on both pull requests and pushes to the repository.\n",
    "on:                                             \n",
    "  - pull_request\n",
    "  - push\n",
    "\n",
    "jobs:\n",
    "\n",
    "  # The test job is defined, and specifies that it should run on different operating systems \n",
    "  # (ubuntu-latest and windows-latest) based on the matrix.os configuration.\n",
    "  test:\n",
    "    runs-on: ${{ matrix.os }}\n",
    "\n",
    "    # The strategy section defines a matrix that includes combinations of \n",
    "    # operating systems and Python versions.\n",
    "    strategy:\n",
    "      matrix:\n",
    "        os: [ubuntu-latest, windows-latest]\n",
    "        python-version: ['3.11.7', '3.12.0']\n",
    "\n",
    "    steps:\n",
    "    # The steps section begins with checking out the repository using the actions/checkout action.\n",
    "    - uses: actions/checkout@v2\n",
    "    \n",
    "    # It then sets up the Python environment using the actions/setup-python action, \n",
    "    # specifying the Python version from the matrix.\n",
    "    - name: Set up Python ${{ matrix.python-version }}\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: ${{ matrix.python-version }}\n",
    "\n",
    "    # This step installs dependencies using the specified Python version. \n",
    "    # It upgrades pip and installs tox and tox-gh-actions\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install tox tox-gh-actions\n",
    "\n",
    "    # The final step runs the tox command, which is a testing tool commonly used in Python projects. \n",
    "    # The tox configuration file (tox.ini) in your project specifies how the tests should be executed.\n",
    "    - name: Test with tox\n",
    "      run: tox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d9ab7-3db7-4771-b3ae-48fd6bf15ce1",
   "metadata": {},
   "source": [
    "## Testing with pytest\n",
    "+ activate your virtual environment\n",
    "    + `$ conda activate example-project`\n",
    "+ make sure your git repo is up-to-date\n",
    "    + `$ git pull`  \n",
    "+ install pytest\n",
    "    + `$ pip install pytest`\n",
    "+ pytest ([https://docs.pytest.org/en/7.1.x/contents.html](https://docs.pytest.org/en/7.1.x/contents.html)) is a python library that allows for very flexible testing of your code.\n",
    "+ In the following I will only provide a small intro into what is possible with pytest\n",
    "\n",
    "### Prepare our Code\n",
    "+ Before we write our first test, we write two new simple functions that compute the mean and variance of a given sample:\n",
    "    + create a new file in the folder example_project called \"expectation\" and open it\n",
    "        + `$ touch example_project/expectation.py`\n",
    "        + `$ start example_project/expectation.py`\n",
    "    + write the following two functions:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53c86214-2638-4159-b8bc-da248aad48fa",
   "metadata": {},
   "source": [
    "# example_project/expectation.py\n",
    "\n",
    "# compute the mean of a sample x\n",
    "def mean(x):\n",
    "    N = len(x)\n",
    "    return 1/N*sum(x)\n",
    "\n",
    "# compute the variance of a sample x \n",
    "def variance(x):\n",
    "    return mean(x**2) - mean(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd18bb8-b806-43e8-ba9a-daba22e0004e",
   "metadata": {},
   "source": [
    "+ now let us write our first test:\n",
    "    + create a new file in our folder `tests` and open it:\n",
    "        + `$ touch tests/test_expectation.py`\n",
    "        + `$ start tests/test_expectation.py`\n",
    "    + write the following lines: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "08891c3e-f011-415f-aa8e-bcf9f742e4de",
   "metadata": {},
   "source": [
    "# tests/test_expectation.py\n",
    "\n",
    "import pytest\n",
    "import example_project.expectation as expectation\n",
    "\n",
    "def test_mean():\n",
    "    pass\n",
    "\n",
    "def test_variance():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932fdac-8f32-4ef5-9c1c-712a86ba5cf7",
   "metadata": {},
   "source": [
    "+ save the file, go to the terminal and type\n",
    "    + `$ cd tests`\n",
    "    + `$ pytest test_expectation.py`\n",
    "+ you should get something similar to:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6658451-9913-4936-acc4-d01d7b67adf7",
   "metadata": {},
   "source": [
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.12.0, pytest-7.1.3, pluggy-1.3.0\n",
    "rootdir: C:\\Users\\flobo\\OneDrive\\Dokumente\\Phd-teaching\\example-project\n",
    "plugins: anyio-4.1.0\n",
    "collected 2 items\n",
    "\n",
    "tests\\test_expectation.py ..                                             [100%]\n",
    "\n",
    "============================== 2 passed in 0.03s =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff895f-041f-4c20-a719-7abe8c1d0d75",
   "metadata": {},
   "source": [
    "+ Thus we have one pass of 100%. This is because we have only one test function and this test function only \"passes\". Thus it cannot fail.\n",
    "+ Let us modify now the test a bit:\n",
    "    + in `test_mean()` we\n",
    "        + provide an input vector [1,2,3] to our function and then\n",
    "        + check whether the result matches with the expected value \"2\" using the `assert` python-keyword\n",
    "    + in `test_variance()` we do basically the same except that we compute the variance by using the numpy.var function. \n",
    "+ Let us run the test again. \n",
    "    + `$ pytest test_expectation.py`  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2d222dd-d1d3-49ca-9c8f-ca957bc555ae",
   "metadata": {},
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import example_project.expectation as expectation\n",
    "\n",
    "def test_mean():\n",
    "    result = expectation.mean(x = np.array([1,2,3]))\n",
    "    assert result == 2\n",
    "\n",
    "def test_variance():\n",
    "    result = expectation.variance(x = np.array([1,2,3]))\n",
    "    assert result == np.var(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658fc06-0e2a-4c20-b04d-6c69a3167284",
   "metadata": {},
   "source": [
    "+ Now, you should get something similar to the following:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01613eda-6085-4530-be96-515d09948b16",
   "metadata": {},
   "source": [
    "============================= test session starts =============================\n",
    "platform win32 -- Python 3.12.0, pytest-7.1.3, pluggy-1.3.0\n",
    "rootdir: C:\\Users\\flobo\\OneDrive\\Dokumente\\Phd-teaching\\example-project\n",
    "plugins: anyio-4.1.0\n",
    "collected 2 items\n",
    "\n",
    "tests\\test_expectation.py .F                                             [100%]\n",
    "\n",
    "================================== FAILURES ===================================\n",
    "________________________________ test_variance ________________________________\n",
    "\n",
    "    def test_variance():\n",
    "        result = expectation.variance(x = np.array([1,2,3]))\n",
    ">       assert result == np.var(np.array([1,2,3]))\n",
    "E       assert 0.6666666666666661 == 0.6666666666666666\n",
    "E        +  where 0.6666666666666666 = <function var at 0x0000023212AC23B0>(array([1, 2, 3]))\n",
    "E        +    where <function var at 0x0000023212AC23B0> = np.var\n",
    "E        +    and   array([1, 2, 3]) = <built-in function array>([1, 2, 3])\n",
    "E        +      where <built-in function array> = np.array\n",
    "\n",
    "tests\\test_expectation.py:11: AssertionError\n",
    "\n",
    "=========================== short test summary info ===========================\n",
    "FAILED tests/test_expectation.py::test_variance - assert 0.6666666666666661 =...\n",
    "================== 1 failed, 1 passed, 32 warnings in 0.35s ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871e5d8-5711-4372-a6ab-ee8375b168f6",
   "metadata": {},
   "source": [
    "+ the second test failed. But why? Remember that we talked about floating point representation and rounding issues that might occur. Pytest hints us already to this fact. We see that our computed variance is almost identical to the compute variance from numpy only in the 16th digit it differs.\n",
    "+ For our current purpose we would rather like that the numbers are treated as equivalent.\n",
    "+ Pytest provides for exactly this situation a function called `pytest.approx()`\n",
    "+ Let's change the code for `test_variance` as follows and run pytest again:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b461515-b590-4bb7-a11a-a955380fd2c2",
   "metadata": {},
   "source": [
    "def test_variance():\n",
    "    result = expectation.variance(x = np.array([1,2,3]))\n",
    "    assert result == pytest.approx(np.var(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6119c3-7a5e-42b4-9080-6de952840ec6",
   "metadata": {},
   "source": [
    "+ all tests should pass now without problems.\n",
    "\n",
    "### Pytest Fixtures\n",
    "+ As you have seen above, we copied the entry [1,2,3]. It would be better to declare it once and then to use the assigned variable where we need it\n",
    "+ For such problems, we can use `pytest.fixture` which is a particular *decorator*\n",
    "\n",
    "#### Small excursus: Decorators\n",
    "+ In Python, a decorator is a design pattern and a special type of syntactic construct that allows you to extend or modify the behavior of functions or methods.\n",
    "+ The decorator itself is a function that takes another function (or method) as its argument.\n",
    "+ The decorator function can modify the behavior of the passed function or perform additional actions before or after its execution.\n",
    "+ Consider the following example:\n",
    "    + we write first a very simple function `hello` and call it\n",
    "    + then we define a new function (`my_decorator`) that modifies the `hello` function (our decorator)\n",
    "    + finally, we add the decorator to the definiton of our `hello` function and call the function again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8deb5d-260a-45eb-baea-cab7f6cc4818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "def hello():\n",
    "    print(\"Hello!\")\n",
    "\n",
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2a6d94-5a40-44bd-9735-ca98dc3b91db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is happening before the function is called.\n",
      "Hello!\n",
      "Something is happening after the function is called.\n"
     ]
    }
   ],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "        func()\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def hello():\n",
    "    print(\"Hello!\")\n",
    "\n",
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ba215-acc1-439a-be71-cb1c8a74d7e4",
   "metadata": {},
   "source": [
    "+ Let us return to our example\n",
    "+ In the following, we want to have one sample from a uniform that we can call in all our tests\n",
    "+ therefore we use a fixture which allows us to share the same information between test definitions\n",
    "+ importantly, we define the scope of the fixture to be `scope = \"class\"` that is the fixture is invoked once per test class."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6905b058-f39c-4280-ab8e-b76bdc8354e4",
   "metadata": {},
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "import example_project.expectation as expectation\n",
    "\n",
    "#%% Test mean\n",
    "@pytest.fixture(scope = \"class\")\n",
    "def x1():\n",
    "    return np.random.uniform(0,1,100)\n",
    "\n",
    "def test_mean(x1):\n",
    "    assert expectation.mean(x1) == pytest.approx(np.mean(x1))\n",
    "\n",
    "#%% Test variance\n",
    "def test_variance(x1):\n",
    "    assert expectation.variance(x1) == pytest.approx(np.var(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c3e34-ecd0-4cba-9c6d-2106ed6035a8",
   "metadata": {},
   "source": [
    "### Another very useful pytest decorator is `@pytest.mark.parameterize`\n",
    "+ pytest.mark.parametrize decorator enables parametrization of arguments for a test function\n",
    "+ allows one to define multiple sets of arguments and fixtures at the test function or class.\n",
    "+ consider we want to test the function `expectation.mean` for negative, positive and mixed values\n",
    "+ we could define three different tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb77bf8-b18d-4428-aa51-8cd5bb2c654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_v1():\n",
    "    x = [2,3]\n",
    "    expected_result = 2.5\n",
    "    assert expectation.mean(x) == expected_result\n",
    "    \n",
    "def test_mean_v2():\n",
    "    x = [-2,-3]\n",
    "    expected_result = -2.5\n",
    "    assert expectation.mean(x) == expected_result\n",
    "    \n",
    "def test_mean_v3():\n",
    "    x = [-2,3]\n",
    "    expected_result = 0.5\n",
    "    assert expectation.mean(x) == expected_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09e6c5-1017-4d13-b7a7-d7996be426c1",
   "metadata": {},
   "source": [
    "+ however, we could also define only one function `test_mean` and then define the vectors as well as the output values as parameters of our test function\n",
    "+ we can use the `@pytest.mark.parameterize` decorator and define each test as a tuple consisting of the vector and the expected result (e.g. `([2,3], 2.5)`)\n",
    "+ the first argument in the parameterize decorator specifies the parameter name of each component in the tuple (e.g., `\"x, expected_result\"`)\n",
    "+ the full test function looks then as follows (note: Pytest runs here three separate tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16e3ec-9435-4d9a-88b0-e9b2602dbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"x, expected_result\", [([2,3], 2.5), ([-2,-3],-2.5), ([-2,3],0.5)])\n",
    "def test_mean2(x, expected_result):\n",
    "    assert expectation.mean(x) == expected_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a5b55-4972-47b4-82ae-ba0423ae8e9e",
   "metadata": {},
   "source": [
    "### Tests for download_data.py (@pytest.fixture, @pytest.mark.parametrize, and mocking)\n",
    "\n",
    "+ Let us run now some tests for the file `download_data.py`\n",
    "+ First, we open the file `example_project\\download_data.py` and make one small change in the function `set_cwd`: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "80a5f5eb-4fb6-41db-ac47-4b158d48a5c9",
   "metadata": {},
   "source": [
    "def set_cwd(self, target_path):\n",
    "    \"\"\"Set current working directory to desired local path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_path : str\n",
    "        local path that should be used as current working directory.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    current_working_directory : str\n",
    "        Prints current working directory.\n",
    "\n",
    "    \"\"\"\n",
    "    # set the current working directory \n",
    "    os.chdir(target_path)\n",
    "    \n",
    "    # get current working directory\n",
    "    current_wd = pathlib.Path.cwd()\n",
    "\n",
    ">>> print(current_wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f5c6e-6948-4967-bd2e-27ceeea808b7",
   "metadata": {},
   "source": [
    "+ save the change and go into the folder `tests`. \n",
    "+ open then file `test_download_data.py`\n",
    "+ First we want to make a test for the function `set_cwd`\n",
    "+ this test should ensure that the set_cwd method of the DownloadData class correctly changes the current working directory to the specified path and that the printed output reflects this change.\n",
    "+ Therefore we do the following:\n",
    "    + This test is using the `download_data_instance` fixture to create an instance of the DownloadData class with a temporary directory for testing.\n",
    "    + The `test_set_cwd` function then calls the set_cwd method of this instance with the argument \"examples.\" (which refers to the \"examples\" folder in our main directory)\n",
    "    + After that, it captures the printed output using the capsys.readouterr() method.\n",
    "    + The test asserts that the printed output (current working directory) ends with the expected path \"examples.\"\n",
    "    + It does this by converting both the actual and expected paths to pathlib.Path objects and comparing the last few components of the paths using the parts attribute."
   ]
  },
  {
   "cell_type": "raw",
   "id": "382dc227-b2e3-47c9-9311-1c004684ef21",
   "metadata": {},
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import pytest\n",
    "from unittest.mock import patch, mock_open\n",
    "from example_project.download_data import DownloadData  \n",
    "\n",
    "@pytest.fixture\n",
    "def download_data_instance(tmp_path):\n",
    "    # Using a temporary directory for testing\n",
    "    data_url = \"http://www.bodowinter.com/tutorial/politeness_data.csv\"  \n",
    "    data_file = tmp_path / \"test_data.csv\"\n",
    "    return DownloadData(data_url=data_url, target_path=str(tmp_path), data_file=str(data_file))\n",
    "\n",
    "def test_set_cwd(download_data_instance, capsys):\n",
    "    # Call the set_cwd method\n",
    "    download_data_instance.set_cwd(\"examples\")\n",
    "\n",
    "    # Capture the printed output\n",
    "    captured = capsys.readouterr()\n",
    "\n",
    "    # Perform assertions on the printed output\n",
    "    expected_path = pathlib.Path(\"examples\")\n",
    "    actual_path = pathlib.Path(captured.out.strip())\n",
    "\n",
    "    # Ensure the actual path ends with the expected path\n",
    "    assert actual_path.parts[-len(expected_path.parts):] == expected_path.parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7c496-8622-4a0c-9042-4548c054d756",
   "metadata": {},
   "source": [
    "+ Second, we want to create a test for the function `download_data`\n",
    "+ This test is essentially checking whether the download_data method works correctly by ensuring that it creates a file with the expected content when given a specific URL. \n",
    "    + Again we create a fixture for the instance of the DownloadData class\n",
    "    + This time, we want to check whether our code retrieves correctly data sets from different urls\n",
    "    +  Using the requests-mock library to mock the download process: The code uses the requests-mock library to mock (=imitate) the download process. The patch function is used to temporarily replace the behavior of requests.get with a mock object. The mock object's content attribute is set to \"mocked data content.\"\n",
    "    +  Assertions: After the download process is mocked and the download_data method is called, the code performs assertions to verify that:\n",
    "        + The file specified by file_name exists.\n",
    "        + The content of the file matches the expected \"mocked data content.\"\n",
    "        + The requests.get method is called exactly once with the provided data_url."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c01d50c-a355-45a3-8613-7fb69c0e8218",
   "metadata": {},
   "source": [
    "@pytest.fixture\n",
    "def download_data_instance(tmp_path):\n",
    "    # Using a temporary directory for testing\n",
    "    data_url = \"http://www.bodowinter.com/tutorial/politeness_data.csv\" \n",
    "    data_file = tmp_path / \"test_data.csv\"\n",
    "    return DownloadData(data_url=data_url, target_path=str(tmp_path), data_file=str(data_file))\n",
    "\n",
    "# Test the download_data method with different URLs\n",
    "@pytest.mark.parametrize(\"data_url\", [\"https://osf.io/download/97g4j/\", \"https://osf.io/download/cvpmr\"])\n",
    "def test_download_data(download_data_instance, data_url, tmp_path):\n",
    "    file_name = tmp_path / \"test_data.csv\"\n",
    "\n",
    "    # Using the requests-mock library to mock the download process\n",
    "    with patch(\"requests.get\") as mock_get:\n",
    "        mock_get.return_value.content = b\"mocked data content\"\n",
    "\n",
    "        download_data_instance.download_data(data_url, file_name)\n",
    "\n",
    "        # Check if the file was created and contains the expected content\n",
    "        assert os.path.exists(file_name)\n",
    "        with open(file_name, \"rb\") as file:\n",
    "            content = file.read()\n",
    "            assert content == b\"mocked data content\"\n",
    "\n",
    "        mock_get.assert_called_once_with(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e862dc-11af-40cd-ac0a-36437e2b0c0b",
   "metadata": {},
   "source": [
    "### Tests for politeness_data.py (@pytest.fixture)\n",
    "\n",
    "Now let us switch to the file `test_politeness_data.py` and make the following small change in the `plot_data` function:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "995f162d-a1c3-49cc-b1b2-f8e67faa3abf",
   "metadata": {},
   "source": [
    "def plot_data(self, df):\n",
    "        \"\"\"Scatterplot showing the relation between pitch in Hz and gender and \n",
    "        attitude.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : polars.DataFrame\n",
    "            Preprocessed data set with 84 observations and 6 variables (subject, \n",
    "            gender, scenario, attitude, frequency, mean_pitch).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        matplotlib.plot\n",
    "\n",
    "        \"\"\"\n",
    "        # create color sequence for grouping variable \n",
    "        # (to be passed into plot)\n",
    "        cols = np.where(df[\"attitude\"] == \"polite\", \"lightblue\", \"orange\")\n",
    "        \n",
    "        # initialie new plot\n",
    "        fig, axs = plt.subplots(constrained_layout = True)\n",
    "        # plot mean values\n",
    "        axs.scatter(df[\"gender\"],df[\"mean_pitch\"], c = cols, \n",
    "                    edgecolors='black')\n",
    "        # plot additionally individual observations\n",
    "        sns.swarmplot(data = pd.DataFrame(df, columns = df.columns),\n",
    "                      x = \"gender\", y = \"frequency\", \n",
    "                      hue = \"attitude\",\n",
    "                      alpha = 0.4, ax=axs)\n",
    "        # relable x-axis\n",
    "        axs.set_xticklabels(labels = [\"Female\", \"Male\"])\n",
    "        axs.set_ylabel(\"pitch in Hz\")\n",
    ">>>>    return (fig, axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d79a73-1d30-4a56-9118-9fb00e5d9764",
   "metadata": {},
   "source": [
    "+ Then go to the `tests` folder and open the file `test_politeness_data.py`\n",
    "+ We will create first a test that ensures that the PolitenessData class, when provided with an example data file, processes the data correctly, and the output has the expected properties.\n",
    "+ Therefore we do the followin:\n",
    "    + **Fixture Setup** (example_data_file):\n",
    "        + A Pytest fixture named `example_data_file` is defined. Fixtures are used to set up resources needed for tests.\n",
    "        + This fixture creates a temporary CSV file (`example_data.csv`) containing sample data related to politeness, using the `tmp_path` fixture to get a temporary directory path.\n",
    "        + The fixture returns the path to the created CSV file as a string.\n",
    "    + **Test Function** (test_data_preprocessed):\n",
    "        + The actual test function is defined. It takes two parameters: example_data_file (the fixture) and capsys (a fixture to capture output created during the test).\n",
    "        + An instance of the `PolitenessData` class is created, initialized with the path to the example data file.\n",
    "        + The `data_preprocessed` method of the PolitenessData class is then called with the example data file.\n",
    "    + **Assertions** are made on the returned values:\n",
    "        + It checks if the result of data_preprocessed is a `polars.DataFrame` (pl.DataFrame), assuming the class uses the Polars library for data manipulation.\n",
    "        + It checks if the summaries variable is also a `polars.DataFrame`.\n",
    "        + It verifies that the `length of the processed data` (df_joined) is 3, assuming there are three rows in the example data.\n",
    "        + It checks that the `length of the summaries` DataFrame is 2, assuming there are two unique combinations of 'attitude' and 'gender' in the example data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f02c422-838f-4da8-9ed5-f849d01d7054",
   "metadata": {},
   "source": [
    "import pytest\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from example_project.politeness_data import PolitenessData\n",
    "\n",
    "@pytest.fixture\n",
    "def example_data_file(tmp_path):\n",
    "    # Create a temporary CSV file with example data\n",
    "    data = {\n",
    "        'subject': [1, 2, 3],\n",
    "        'gender': ['male', 'female', 'male'],\n",
    "        'scenario': ['A', 'B', 'A'],\n",
    "        'attitude': ['pol', 'inf', 'pol'],\n",
    "        'frequency': [100, 150, 120],\n",
    "        'mean_pitch': [200, 180, 220]\n",
    "    }\n",
    "    file_path = tmp_path / \"example_data.csv\"\n",
    "    pd.DataFrame(data).to_csv(file_path, index=False)\n",
    "    return str(file_path)\n",
    "\n",
    "def test_data_preprocessed(example_data_file, capsys):\n",
    "    # Initialize the PolitenessData class with the example data file\n",
    "    politeness_data = PolitenessData(data_file=example_data_file)\n",
    "\n",
    "    # Call the data_preprocessed method\n",
    "    df_joined, summaries = politeness_data.data_preprocessed(example_data_file)\n",
    "\n",
    "    # Perform assertions on the processed data\n",
    "    assert isinstance(df_joined, pl.DataFrame)\n",
    "    assert isinstance(summaries, pl.DataFrame)\n",
    "    assert len(df_joined) == 3  # Assuming three rows in the example data\n",
    "    assert len(summaries) == 2  # Assuming two unique combinations of attitude and gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6f8db-f605-4538-a5be-7b1f88bda9fa",
   "metadata": {},
   "source": [
    "+ Finally, we also add a test for the function `plot_data`\n",
    "+ The test should ensure that the plot_data method in the PolitenessData class behaves as expected.\n",
    "+ Specifically, it should verify that the method returns a Matplotlib Axes object and that the generated plot has the expected number of legend items.\n",
    "+ Therefore, we do the following:\n",
    "  + **Fixture Setup** (example_data_file and capsys):\n",
    "      + Similar to the previous example, the `example_data_file` fixture is used to create a temporary CSV file with example data.\n",
    "      + The capsys fixture is used to capture the output during the test.\n",
    "\n",
    "  + **Test Function** (test_plot_data):\n",
    "       + An `instance of the PolitenessData` class is created and initialized with the path to the example data file.\n",
    "       + The `data_preprocessed` method of the PolitenessData class is called to obtain processed data (df_joined).\n",
    "       + The `plot_data` method is then called with the processed data, and the result is captured in the axs variable.\n",
    "  + **Assertions** are made on the results:\n",
    "       + It checks if the `axs` variable is an instance of plt.Axes, indicating that the method returns a Matplotlib Axes object.\n",
    "       + It checks that the number of legend items in the plot is 2, assuming there are two items in the legend. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "32623049-d6dc-40a9-b99f-e9cb3ad63b88",
   "metadata": {},
   "source": [
    "def test_plot_data(example_data_file, capsys):\n",
    "    # Initialize the PolitenessData class with the example data file\n",
    "    politeness_data = PolitenessData(data_file=example_data_file)\n",
    "\n",
    "    # Call the data_preprocessed method to get the processed data\n",
    "    df_joined, _ = politeness_data.data_preprocessed(example_data_file)\n",
    "\n",
    "    # Call the plot_data method\n",
    "    _, axs = politeness_data.plot_data(df_joined)\n",
    "\n",
    "    # Perform assertions on the results\n",
    "    assert isinstance(axs, plt.Axes)\n",
    "\n",
    "    # Additional assertions for the plot characteristics\n",
    "    assert len(axs.get_legend().get_texts()) == 2  # Expecting two legend items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e1396-768c-4374-8c02-36ed5a0fb343",
   "metadata": {},
   "source": [
    "### @pytest.mark.skip and @pytest.mark.xfail\n",
    "\n",
    "+ You have a lot of different opportunities with pytest.\n",
    "+ Among others it is also possible to skip certain test or to `xfail` them (that is: you already know that this test will fail but have no fix for the problem at the moment)\n",
    "+ In the following two examples for each case:                                                                                          "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d197f5e-8199-4188-83da-67edff6369fe",
   "metadata": {},
   "source": [
    "@pytest.mark.skip(reason=\"no way of currently testing this\")\n",
    "def test_the_unknown():\n",
    "    ...\n",
    "\n",
    "--\n",
    "\n",
    "@pytest.mark.xfail(reason=\"known parser issue\")\n",
    "def test_function():\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9a109-07a7-400b-9a34-15be11985640",
   "metadata": {},
   "source": [
    "## Using tox for testing multiple environments\n",
    "+ [tox](https://tox.wiki/en/4.11.4/) is a generic virtual environment management and test command line tool which aims to automate and standardize testing in Python\n",
    "+ Let's first install tox:\n",
    "    + `pip install tox`\n",
    "+ Then we can run tox (make sure you are in the main directory of your package)\n",
    "    + `tox`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e99380-24cb-445d-94d4-19790f055ac3",
   "metadata": {},
   "source": [
    "## Adding test badge to README.md\n",
    "\n",
    "+ finally, let us create in our README.md also a \"badge\" for our tests\n",
    "+ open the `README.md` and add the following line:\n",
    "    + change \"USERNAME\" with your personal GitHub user name "
   ]
  },
  {
   "cell_type": "raw",
   "id": "952a248a-a1b2-49e3-b477-41f154afbbfd",
   "metadata": {},
   "source": [
    "![Tests](https://github.com/USERNAME/example-project/actions/workflows/tests.yml/badge.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d88949-cfca-47e1-b56c-04d92ceb2989",
   "metadata": {},
   "source": [
    "+ now we can add, commit, and push our changes to our Github repo\n",
    "    + `$ git add --all`\n",
    "    + `$ git commit -m \"add tests\"`\n",
    "    + `$ git push`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042af824-ffca-46b4-bad1-d56c33a84aca",
   "metadata": {},
   "source": [
    "let's open our GitHub repo site and check whether all deployment stages work properly and the tests pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f192e-029f-47e0-9310-a8b42ff94d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
